import copy
import logging
from abc import ABC, abstractmethod
from fractions import Fraction
from typing import Callable, List, Dict
from pathlib import Path
import pprint

import z3
import dreal
import cvc5.pythonic as cvc5
import numpy as np
import sympy as sp
import torch
import torch.nn as nn

from maraboupy import Marabou
from maraboupy import MarabouCore

from envs import Box, Env

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


class Verifier(ABC):
  @abstractmethod
  def __init__(
      self,
      models: List[nn.Sequential],
      env: Env,
      F: Callable
  ):
    ...

  @abstractmethod
  def chk(self) -> List[List]:
    ...


# Representation of ReLU in SymPy
ReLU = sp.Function('ReLU')


def ColumnVector(pattern: str, dim: int):
  """Representation of a column vector named pattern in SymPy."""
  return sp.Matrix(
    [sp.Symbol(f'{pattern}{i}') for i in range(dim)])


def Wb(lyr: nn.Linear, numpy=True):
  """Weight (W) and bias (b) of an nn.Linear layer.

  If lyr has no bias (e.g., by setting bias=False in constructor),
  then a zero Tensor will be returned as bias.
  """
  W = lyr.weight.data
  b = None
  if lyr.bias is not None:
    b = lyr.bias.data
  else:
    b = torch.zeros(lyr.out_features)
  assert b is not None
  b = torch.unsqueeze(b, 1)
  if numpy:
    W, b = W.numpy(), b.numpy()
  return W, b


def Net(net: nn.Sequential, x: sp.Matrix, netname='y'):
  """Representation of a ReLU-activated NN in SymPy.
  
  Args:
    net: an instance of nn.Sequential. We assume all layers are 
    instances of either nn.Linear (fully connected feed-forward) or 
    nn.ReLU (activation functions).
    x: input matrix.
    netname: name of the network. This name is used to name all
    constraint variables and, consequently, output variables.
  Returns:
    output: a SymPy Matrix of symbols from the last layer.
    constraints: a list of SymPy expressions. All expressions are of
    the form Eq(..., ...).
  """

  output = None
  constraints = []
  for i in range(len(net)):
    layer = net[i]
    match layer:
      case nn.Linear():
        output = ColumnVector(
          f'{netname}_{i},', layer.out_features)
        W, b = Wb(net[i])
        result = W @ x + b
        for j in range(layer.out_features):
          constraints.append(sp.Eq(output[j], result[j]))
        # Setting output of current layer as input for the next.
        x = output
      case nn.ReLU():
        output = ColumnVector(f'{netname}_{i},', len(x))
        for j in range(len(x)):
          constraints.append(sp.Eq(output[j], ReLU(x[j])))
        x = output
  assert output is not None
  return output, constraints


def BoundIn(box: Box, x: sp.Matrix):
  """Representation of the points bound inside a Box in SymPy.

  Returns:
    constrains: a list of SmyPy expressions. All expressions are of
    the form ... >= ... or ... <= ... .
  """
  constraints = []
  dim = len(box.low)
  low = box.low.numpy()
  high = box.high.numpy()
  constraints += [x[i] >= low[i] for i in range(dim)]
  constraints += [x[i] <= high[i] for i in range(dim)]
  return constraints


def BoundOut(box: Box, x):
  """Representation of the points outside a Box in SymPy.

    Returns:
      constrains: a list containing exactly one SmyPy expression, of
      the form Or(...).
    """
  constraints = []
  dim = len(box.low)
  low = box.low.numpy()
  high = box.high.numpy()
  constraints += [x[i] <= low[i] for i in range(dim)]
  constraints += [x[i] >= high[i] for i in range(dim)]
  return [sp.Or(*constraints)]


def Norm_L1(x):
  # || x ||_1 = [1 ... 1] * Abs(x). We take the only element
  # in the 1x1 result Matrix.
  norm = sp.ones(1, len(x)) @ x.applyfunc(sp.Abs)
  return norm[0]


def solve(constraints: List, x: List, solver=z3):
  """Satisfying model for constraints.
  If a model exists for C, this function returns a list 
  [y_1, ..., y_k], where y_i = model[x_i].
  
  Assumption. All variables in x are Real.
  
  Args:
    constraints: A list of Z3 constraints.
    x: A list of Z3 variables.
    solver: Name of the solver module. This argument should be
    exactly either z3 or cvc5.
  """
  s = solver.Solver()
  s.add(constraints)
  chk = s.check()
  if chk == solver.sat:
    m = s.model()
    n = len(x)
    return [float(m[x[i]].as_fraction()) for i in range(n)]
  elif chk == solver.unsat:
    return []
  else:
    raise RuntimeError('unknown result for SMT query')


def solve_dreal(formula, x, delta=1e-3):
  """Satisfying model for formula, generated by DReal."""
  result = dreal.CheckSatisfiability(formula, delta)
  if not result:
    return []
  # logger.debug('DReal result = ')
  # print(result)
  return [result[x[i]].mid() for i in range(len(x))]


def sympy_to_z3(expr, var):
  """Translate SymPy expression to Z3.

  Args:
    expr: SymPy expression.
    var: a dictionary mapping SymPy Symbols to their Z3 equivalents. 
  """
  match expr:
    case sp.Symbol():
      return var[expr]
    case sp.Number():
      return expr
    case sp.Add():
      acc = sympy_to_z3(expr.args[0], var)
      for arg in expr.args[1:]:
        acc += sympy_to_z3(arg, var)
      return acc
    case sp.Mul():
      acc = sympy_to_z3(expr.args[0], var)
      for arg in expr.args[1:]:
        acc *= sympy_to_z3(arg, var)
      return acc
    case sp.And():
      args = [sympy_to_z3(arg, var) for arg in expr.args]
      return z3.And(args)
    case sp.Or():
      args = [sympy_to_z3(arg, var) for arg in expr.args]
      return z3.Or(args)
    case sp.Abs():
      arg = sympy_to_z3(expr.args[0], var)
      return z3.If(arg < 0, -arg, arg)
    case sp.Function() if expr.name == 'ReLU':
      arg = sympy_to_z3(expr.args[0], var)
      return z3.If(arg < 0, 0, arg)
    case sp.GreaterThan():
      l, r = [sympy_to_z3(arg, var) for arg in expr.args]
      return l >= r
    case sp.LessThan():
      l, r = [sympy_to_z3(arg, var) for arg in expr.args]
      return l <= r
    case sp.StrictGreaterThan():
      l, r = [sympy_to_z3(arg, var) for arg in expr.args]
      return l > r
    case sp.StrictLessThan():
      l, r = [sympy_to_z3(arg, var) for arg in expr.args]
      return l < r
    case sp.Eq():
      left, right = [sympy_to_z3(arg, var) for arg in expr.args]
      return left == right
    case _:
      raise NotImplementedError(type(expr))


def sympy_to_cvc5(expr, var):
  """Translate SymPy expression to CVC5.

  Args:
    expr: SymPy expression.
    var: mapping from SymPy Symbols to CVC5 Reals
  """
  match expr:
    case sp.Symbol():
      return var[expr]
    case sp.Number():
      return Fraction(str(expr))
    case sp.Add():
      acc = sympy_to_cvc5(expr.args[0], var)
      for arg in expr.args[1:]:
        acc += sympy_to_cvc5(arg, var)
      return acc
    case sp.Mul():
      acc = sympy_to_cvc5(expr.args[0], var)
      for arg in expr.args[1:]:
        acc *= sympy_to_cvc5(arg, var)
      return acc
    case sp.And():
      args = [sympy_to_cvc5(arg, var) for arg in expr.args]
      return cvc5.And(*args)
    case sp.Or():
      args = [sympy_to_cvc5(arg, var) for arg in expr.args]
      return cvc5.Or(*args)
    case sp.Abs():
      arg = sympy_to_cvc5(expr.args[0], var)
      return cvc5.If(arg < 0, -arg, arg)
    case sp.Function() if expr.name == 'ReLU':
      arg = sympy_to_cvc5(expr.args[0], var)
      return cvc5.If(arg < 0, 0, arg)
    case sp.GreaterThan():
      left, right = [sympy_to_cvc5(arg, var) for arg in expr.args]
      return left >= right
    case sp.LessThan():
      left, right = [sympy_to_cvc5(arg, var) for arg in expr.args]
      return left <= right
    case sp.StrictGreaterThan():
      left, right = [sympy_to_cvc5(arg, var) for arg in expr.args]
      return left > right
    case sp.StrictLessThan():
      left, right = [sympy_to_cvc5(arg, var) for arg in expr.args]
      return left < right
    case sp.Eq():
      left, right = [sympy_to_cvc5(arg, var) for arg in expr.args]
      return left == right
    case _:
      raise NotImplementedError(type(expr))


def sympy_to_dreal(expr, var):
  """Translate SymPy expression to DReal.

  Args:
    expr: SymPy expression.
    var: mapping from SymPy Symbols to DReal Variables
  """
  match expr:
    case sp.Symbol():
      return var[expr]
    case sp.Number():
      return expr
    case sp.Add():
      acc = sympy_to_dreal(expr.args[0], var)
      for arg in expr.args[1:]:
        acc += sympy_to_dreal(arg, var)
      return acc
    case sp.Mul():
      acc = sympy_to_dreal(expr.args[0], var)
      for arg in expr.args[1:]:
        acc *= sympy_to_dreal(arg, var)
      return acc
    case sp.And():
      args = [sympy_to_dreal(arg, var) for arg in expr.args]
      return dreal.And(*args)
    case sp.Or():
      args = [sympy_to_dreal(arg, var) for arg in expr.args]
      return dreal.Or(*args)
    case sp.GreaterThan():
      left, right = [sympy_to_dreal(arg, var) for arg in expr.args]
      return left >= right
    case sp.LessThan():
      left, right = [sympy_to_dreal(arg, var) for arg in expr.args]
      return left <= right
    case sp.StrictGreaterThan():
      left, right = [sympy_to_dreal(arg, var) for arg in expr.args]
      return left > right
    case sp.StrictLessThan():
      left, right = [sympy_to_dreal(arg, var) for arg in expr.args]
      return left < right
    case sp.Eq():
      left, right = [sympy_to_dreal(arg, var) for arg in expr.args]
      return left == right
    # Functions
    case sp.Abs():
      arg = sympy_to_dreal(expr.args[0], var)
      return dreal.if_then_else(arg > 0, arg, 0)
    case sp.sin():
      arg = sympy_to_dreal(expr.args[0], var)
      return dreal.sin(arg)
    case sp.cos():
      arg = sympy_to_dreal(expr.args[0], var)
      return dreal.cos(arg)
    case sp.exp():
      arg = sympy_to_dreal(expr.arg[0], var)
      return dreal.exp(arg)
    case sp.Function() if expr.name == 'ReLU':
      arg = sympy_to_dreal(expr.args[0], var)
      return dreal.if_then_else(arg < 0, 0, arg)
    case _:
      raise NotImplementedError(type(expr))


# Verifiers are named after their corresponding learners. Concretely, 
# Verifier_W (where W is a string) corresponds to Learner_W.

class Verifier_Reach_V(Verifier):
  def __init__(self, models, env, F):
    self.V = models[0]
    self.env = env
    self.F = F

  def chk(self):
    cex = self.chk_dec()
    return cex if len(cex) != 0 else []

  def chk_dec(self):
    logger.info('Checking the Decrease condition ...')
    dim = self.env.dim
    x = ColumnVector('x_', dim)

    bounds, problem = [], []
    bounds += BoundIn(self.env.bnd, x)
    bounds += BoundOut(self.env.tgt, x)

    # V(x) <= V(F(x))
    v_o, v_cs = Net(self.V, x, 'V')
    f_o, f_cs = self.F(x)
    vf_o, vf_cs = Net(self.V, f_o, 'VF')
    problem += v_cs
    problem += f_cs
    problem += vf_cs
    problem.append(v_o[0] <= vf_o[0])
    logger.debug('bounds=' + pprint.pformat(bounds))
    logger.debug('problem=' + pprint.pformat(problem))
    constraints = bounds + problem

    x_dreal = [dreal.Variable(f'x_{i}') for i in range(dim)]
    var = [c.atoms(sp.Symbol) for c in constraints]
    var = set().union(*var)
    var = {v: dreal.Variable(v.name) for v in var}
    constraints = [sympy_to_dreal(c, var) for c in constraints]
    formula = dreal.And(*constraints)
    return solve_dreal(formula, x_dreal)


class ABVComposite(nn.Module):
  """Composite network containing A, B, and V. This network takes
  (x, y) as (a single) input, where x is a sample from the state
  space and y is an error variable, and returns the following as
  output: (
    V(x) - V(A(x) + y),
    ||y||_1 - B(x)
  )

  This network consists of A, B, and V along with paddings (identity
  function) and simple operations such as addition, and L1-norm
  computation. The resulting network shall look like a simple neural
  network with Linear and ReLU layers.

  Assumption. X and y are passed concatenated together and as a
  2D-Tensor with only one row.  Output is also a 2D-Tensor with only
  one row and two columns.
  """

  def __init__(self, A, B, V, dim):
    super().__init__()
    self.A = A
    self.B = B
    self.V = V
    self.V1 = copy.deepcopy(V)
    self.I_x = self.identity_relu(dim)
    self.I_y = self.identity_relu(dim)
    self.L1Norm_y = self.l1norm(dim)
    self.Composite = self.build(dim)

  @staticmethod
  def identity_relu(dim):
    """Identity NN with ReLU activation layers, which takes input (x)
    with dimensionality dim, and returns (x). The returned NN has
    layer structure (Linear, ReLU, Linear).
    """
    net = nn.Sequential(
      nn.Linear(dim, 2 * dim, bias=False),
      nn.ReLU(),
      nn.Linear(2 * dim, dim, bias=False),
    )
    with torch.no_grad():
      I_ = torch.eye(dim, dim)
      net[0].weight = nn.Parameter(torch.vstack((I_, -I_)))
      net[2].weight = nn.Parameter(torch.hstack((I_, -I_)))
    return net

  @staticmethod
  def identity(dim):
    """Identity NN that takes input (x) with dimensionality dim, and
    returns (x). The returned NN has layer structure (Linear).
    """
    net = nn.Linear(dim, dim, bias=False)
    with torch.no_grad():
      I_ = torch.eye(dim, dim)
      net.weight = nn.Parameter(I_)
    return net

  @staticmethod
  def broadcast(dim, k) -> nn.Linear:
    """Broadcaster NN that takes input (x) with dimensionality dim,
    and returns (x, ... x) (k times) of dimensionality k*dim.

    Args:
      dim: dimensionality of input.
      k: broadcasting degree.
    """
    net = nn.Linear(dim, dim * k, bias=False)
    with torch.no_grad():
      I_ = torch.eye(dim, dim)
      W = I_
      for i in range(k-1):
        W = torch.vstack((W, I_))
      net.weight = nn.Parameter(W)
    return net

  @staticmethod
  def permute(dim, n, p) -> nn.Linear:
    """Permutation NN that takes input (x_1, ..., x_n), where each
    x_i is of dimensionality dim, and returns (x_p_1, ..., x_p_n),
    i.e., a permutation of (x_1, ..., x_n).

    Args:
      dim: dimensionality of each x_i.
      n: number of input values.
      p: permutation (i.e., a list containing all integers in
      [0, n-1]).
    """
    net = nn.Linear(n*dim, n*dim, bias=False)
    with torch.no_grad():
      W = torch.zeros(n*dim, n*dim)
      for i in range(n):
        r, c = dim*i, dim*p[i]
        W[r:r+dim, c:c+dim] = torch.eye(dim, dim)
      net.weight = nn.Parameter(W)
    return net

  @staticmethod
  def add(dim) -> nn.Linear:
    """Adder NN that takes inputs (x, y) with equal
    dimensionality dim, and returns x+y (vector sum of x and y).

    Args:
      dim: dimensionality of each operand.
    """
    net = nn.Linear(2*dim, dim, bias=False)
    with torch.no_grad():
      I_ = torch.eye(dim, dim)
      W = torch.hstack((I_, I_))
      net.weight = nn.Parameter(W)
    return net

  @staticmethod
  def sub(dim) -> nn.Linear:
    """Subtractor NN that takes inputs (x, y) with equal
    dimensionality dim, and returns x-y (vector sum of x and -y).

    Args:
      dim: dimensionality of each operand.
    """
    net = nn.Linear(2 * dim, dim, bias=False)
    with torch.no_grad():
      I_ = torch.eye(dim, dim)
      W = torch.hstack((I_, -I_))
      net.weight = nn.Parameter(W)
    return net

  @staticmethod
  def l1norm(dim):
    """L1-Norm NN that takes input (x) with dimensionality dim, and
    returns ||x||_1 (L1-Norm of x).

    Args:
      dim: dimensionality of x.
    """
    net = nn.Sequential(
      nn.Linear(dim, 2*dim, bias=False),
      nn.ReLU(),
      nn.Linear(2*dim, 1, bias=False),
      nn.ReLU(),
    )
    with torch.no_grad():
      I_ = torch.eye(dim, dim)
      # Abs(x) = ReLU(x) + ReLU(-x).
      # Net[0]: (x) -> (x, -x)
      # Net[1]: (x, -x) -> (ReLU(x), ReLU(-x))
      # Net[2]: (ReLU(x), ReLU(-x)) = (ReLU(x) + ReLU(-x))
      net[0].weight = nn.Parameter(torch.vstack((I_, -I_)))
      net[2].weight = nn.Parameter(torch.ones(1, 2*dim))
    return net

  @staticmethod
  def vstack(layers: List[nn.Linear]) -> nn.Linear:
    """Vertically stack a list of nn.Linear layers.

    Args:
      layers: list of nn.Linear-s.
    """
    in_ = [layer.in_features for layer in layers]
    out = [layer.out_features for layer in layers]
    result = nn.Linear(sum(in_), sum(out))
    with torch.no_grad():
      Wbs = [Wb(layer, numpy=False) for layer in layers]
      Ws, bs = list(zip(*Wbs))
      b = torch.cat(bs)
      b = torch.squeeze(b, 1)
      result.bias = nn.Parameter(b)
      W_result = torch.Tensor(0, sum(in_))
      for i in range(len(layers)):
        W_padded = (
          torch.zeros(out[i], sum(in_[:i])),
          Ws[i],
          torch.zeros(out[i], sum(in_[i+1:]))
        )
        W_padded = torch.hstack(W_padded)
        W_result = torch.vstack((W_result, W_padded))
      result.weight = nn.Parameter(W_result)
    return result

  def build(self, dim):
    # Check variables.
    x, y = torch.rand(dim), torch.rand(dim)
    xy = torch.cat((x, y))
    result = nn.Sequential()
    # Input layout = (x, y)
    result.append(
      self.vstack(
        [self.broadcast(dim, 2), self.identity(dim)]
      )
    )
    # Input layout = (x, x, y)
    assert torch.equal(result(xy), torch.cat((x, x, y)))
    # Assertion. A, I_x, and I_y are all structured as
    # (Linear, ReLU, Linear).
    assert all(
      isinstance(l0, nn.Linear)
      for l0 in (self.A[0], self.I_x[0], self.I_y[0])
    ) and all(
      isinstance(l1, nn.ReLU)
      for l1 in (self.A[1], self.I_x[1], self.I_y[1])
    ) and all(
      isinstance(l2, nn.Linear)
      for l2 in (self.A[2], self.I_x[2], self.I_y[2])
    )
    result.append(
      self.vstack([self.A[0], self.I_x[0], self.I_y[0]])
    )
    result.append(nn.ReLU())
    result.append(
      self.vstack([self.A[2], self.I_x[2], self.I_y[2]])
    )
    # Input layout = (A(x), x, y)
    assert torch.equal(result(xy), torch.cat((self.A(x), x, y)))
    result.append(self.permute(dim, 3, [0, 2, 1]))
    # Input layout = (A(x), y, x)
    assert torch.equal(result(xy), torch.cat((self.A(x), y, x)))
    result.append(
      self.vstack([
        self.identity(dim),
        self.broadcast(dim, 2),
        self.identity(dim),
      ])
    )
    # Input layout = (A(x), y, y, x)
    assert torch.equal(result(xy), torch.cat((self.A(x), y, y, x)))
    result.append(
      self.vstack(
        [self.add(dim), self.identity(dim), self.identity(dim)]
      )
    )
    # Input layout = (A(x)+y, y, x)
    assert torch.equal(result(xy), torch.cat((self.A(x)+y, y, x)))
    result.append(
      self.vstack([
        self.identity(dim),
        self.identity(dim),
        self.broadcast(dim, 2),
      ])
    )
    # Input layout = (A(x)+y, y, x, x)
    assert torch.equal(result(xy), torch.cat((self.A(x)+y, y, x, x)))
    result.append(self.permute(dim, 4, [2, 0, 1, 3]))
    # Input layout = (x, A(x)+y, y, x)
    assert torch.equal(result(xy), torch.cat((x, self.A(x)+y, y, x)))
    # Assertion. V, V1, B, and L1_Norm_y are all structured as
    # (Linear, ReLU, Linear, ReLU).
    assert all(
      isinstance(l0, nn.Linear)
      for l0 in (self.V[0], self.V1[0], self.B[0], self.L1Norm_y[0])
    ) and all(
      isinstance(l1, nn.ReLU)
      for l1 in (self.V[1], self.V1[1], self.B[1], self.L1Norm_y[1])
    ) and all(
      isinstance(l2, nn.Linear)
      for l2 in (self.V[2], self.V1[2], self.B[2], self.L1Norm_y[2])
    ) and all(
      isinstance(l1, nn.ReLU)
      for l1 in (self.V[3], self.V1[3], self.B[3], self.L1Norm_y[3])
    )
    result.append(
      self.vstack([
        self.V[0], self.V1[0], self.L1Norm_y[0], self.B[0]
      ])
    )
    result.append(nn.ReLU())
    result.append(
      self.vstack([
        self.V[2], self.V1[2], self.L1Norm_y[2], self.B[2]
      ])
    )
    result.append(nn.ReLU())
    # Input layout = (V(x), V(A(x)+y), ||y||_1, B(x))
    assert torch.equal(
      result(xy),
      torch.cat((
        self.V(x),
        self.V(self.A(x)+y),
        torch.unsqueeze(torch.sum(torch.abs(y)), 0),
        self.B(x)
      ))
    )
    result.append(self.vstack([self.sub(1), self.sub(1)]))
    # Input layout = (V(x) - V(A(x)+y), ||y||_1 - B(x))
    assert torch.equal(
      result(xy),
      torch.cat((
        self.V(x) - self.V(self.A(x)+y),
        torch.unsqueeze(torch.sum(torch.abs(y)), 0) - self.B(x)
      ))
    )
    return result

  def forward(self, xy):
    return self.Composite(xy)


class Verifier_Reach_ABV(Verifier):
  def __init__(self, models, env, F):
    self.A, self.B, self.V = models
    self.env = env
    self.F = F

  def chk(self):
    cexs = [self.chk_abst(), self.chk_dec()]
    return [cex for cex in cexs if len(cex) != 0]

  def chk_abst(self):
    """Check the Abstraction-Bound condition using DReal.

    As this check involves a possibly nonlinear function F (hence,
    an NRA query), this method always uses DReal, and need not be
    implemented in subclasses of Verifier_Reach_ABV.
    """
    logger.info('Checking the Abstraction-Bound condition ...')
    dim = self.env.dim
    x = ColumnVector('x_', self.env.dim)

    bounds = BoundIn(self.env.bnd, x)
    problem = []
    # err = || A(x) - f(x) ||_1
    a_o, a_cs = Net(self.A, x, 'A')
    problem += a_cs
    f_o, f_cs = self.F(x)
    err = sp.Symbol('err')
    problem.append(sp.Eq(err, Norm_L1(a_o - f_o)))
    # b = B(x). We need to reshape b to be a scalar, rather than a
    # matrix with only one element.
    b_o, b_cs = Net(self.B, x, 'B')
    problem += b_cs
    assert b_o.shape == (1, 1)
    b_o = b_o[0]
    problem.append(err > b_o)
    logger.debug('bounds=' + pprint.pformat(bounds))
    logger.debug('problem=' + pprint.pformat(problem))

    x_dreal = [dreal.Variable(f'x_{i}') for i in range(dim)]
    constraints = bounds + problem
    var = [c.atoms(sp.Symbol) for c in constraints]
    var = set().union(*var)
    var = {v: dreal.Variable(v.name) for v in var}
    # logger.debug(var)
    constraints = [sympy_to_dreal(c, var) for c in constraints]
    formula = dreal.And(*constraints)
    return solve_dreal(formula, x_dreal)

  def dec_constrains(self):
    """Symbolic constrains for encoding the Decrease Condition."""
    dim = self.env.dim
    x = ColumnVector('x_', dim)
    y = ColumnVector('y_', dim)

    # Both bounds and problem will be kept symbolic (i.e., SymPy
    # expressions) until just before calling the solver.
    bounds, problem = [], []
    bounds += BoundIn(self.env.bnd, x)
    bounds += BoundOut(self.env.tgt, x)

    # ||y||_1 <= B(x)
    norm_y = sp.Symbol('||y||_1')
    problem.append(sp.Eq(norm_y, Norm_L1(y)))
    b_o, b_cs = Net(self.B, x, 'B')
    problem += b_cs
    problem.append(norm_y <= b_o[0])

    # V(x) <= V( A(x) + y) )
    v_o, v_cs = Net(self.V, x, 'V')
    a_o, a_cs = Net(self.A, x, 'A')
    va_o, va_cs = Net(self.V, a_o + y, 'VA')
    problem += v_cs
    problem += a_cs
    problem += va_cs
    problem.append(v_o[0] <= va_o[0])
    logger.debug('bounds=' + pprint.pformat(bounds))
    logger.debug('problem=' + pprint.pformat(problem))
    constraints = bounds + problem
    return constraints

  @abstractmethod
  def chk_dec(self):
    ...


class Verifier_Reach_ABV_Marabou(Verifier_Reach_ABV):
  def chk_dec(self):
    abv = ABVComposite(
      self.A, self.B, self.V, self.env.dim)
    dim = self.env.dim
    xy = torch.randn(1, 2*dim)

    filename = 'marabou_drafts/abv.onnx'
    torch.onnx.export(
      abv, xy, filename,
      input_names=['xy'],
      output_names=['o'])

    network = Marabou.read_onnx(filename)
    # Path(filename).unlink()

    xy = network.inputVars[0][0]
    o = network.outputVars[0][0]
    logger.debug(f'x = {xy}')
    logger.debug(f'o = {o}')

    bnd = self.env.bnd
    dim = self.env.dim
    low = bnd.low.numpy()
    high = bnd.high.numpy()
    # Bounding y as well to avoid having infinite bounds.
    for i in range(2*dim):
      network.setLowerBound(xy[i], low[i % dim])
      network.setUpperBound(xy[i], high[i % dim])
      network.setLowerBound(xy[i], low[i % dim])
      network.setUpperBound(xy[i], high[i % dim])

    # Bounding x to not be in the target region.
    tgt = self.env.tgt
    low = tgt.low.numpy()
    high = tgt.high.numpy()
    for i in range(dim):
      # eq1. 1 * x[i] >= high[i]
      eq1 = MarabouCore.Equation(MarabouCore.Equation.GE)
      eq1.addAddend(1, xy[i])
      eq1.setScalar(high[i])
      # eq2. 1 * x[i] <= low[i]
      eq2 = MarabouCore.Equation(MarabouCore.Equation.LE)
      eq2.addAddend(1, xy[i])
      eq2.setScalar(low[i])
      # eq1 \/ eq2
      network.addDisjunctionConstraint([[eq1], [eq2]])

    network.setUpperBound(o[0], 0.0)
    network.setUpperBound(o[1], 0.0)
    network.setLowerBound(o[0], -1e3)
    network.setLowerBound(o[1], -1e3)

    network.saveQuery('marabou_drafts/abv-query.txt')
    options = Marabou.createOptions(
      verbosity=2,
      tighteningStrategy='none',
    )
    chk, vals, _stats = network.solve(options=options)
    if chk == 'sat':
      return [vals[xy[i]] for i in range(dim)]
    return None


class Verifier_Reach_ABV_Z3(Verifier_Reach_ABV):
  def chk_dec(self):
    logger.info('Checking the Decrease condition ...')
    constraints = self.dec_constrains()
    var = [c.atoms(sp.Symbol) for c in constraints]
    var = set().union(*var)
    var = {v: z3.Real(v.name) for v in var}

    constraints = [sympy_to_z3(c, var) for c in constraints]
    x_z3 = [z3.Real(f'x_{i}') for i in range(self.env.dim)]
    return solve(constraints, x_z3, z3)


class Verifier_Reach_ABV_CVC5(Verifier_Reach_ABV):
  def chk_dec(self):
    logger.info('Checking the Decrease condition ...')
    constraints = self.dec_constrains()
    var = [c.atoms(sp.Symbol) for c in constraints]
    var = set().union(*var)
    var = {v: cvc5.Real(v.name) for v in var}

    constraints = [sympy_to_cvc5(c, var) for c in constraints]
    x_cvc5 = [cvc5.Real(f'x_{i}') for i in range(self.env.dim)]
    return solve(constraints, x_cvc5, cvc5)
